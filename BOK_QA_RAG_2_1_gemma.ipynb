{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ysys143/ml2024/blob/main/BOK_QA_RAG_2_1_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBhIeZe45KHM"
      },
      "source": [
        "# Model Outline\n",
        "\n",
        "Made by Jiwoo Kim, Jihoon Park, Jaesol Shin\n",
        "\n",
        "## 환경설정\n",
        "\n",
        "1) 실행 전 도구-명령팔레트-사용자 보안 비밀 탭 열기에서 HF_TOKEN의 값을 설정해주세요.\n",
        "\n",
        "2) 지정된 위치에 폴더를 만들어 Q&A, 연차보고서 텍스트 파일을 넣어주세요.\n",
        "\n",
        "path = '/content/drive/MyDrive/Google Playground/'\n",
        "\n",
        "## Q&A\n",
        "Encoder : Sentence-BERT 기반\n",
        "Knowledge Source : BoK Q&A<br>\n",
        "Finetuned by Q&A and 2019-2023 annaual reports\n",
        "jiuuu26/chat-BOK -> roomnumber103/embedding-BOK<br>\n",
        "\n",
        "## RAG\n",
        "Knowledge Source : 2019-2023 annaual reports<br>\n",
        "Chungking : RecursiveCharacterTextSplitter<br>\n",
        "Retriever : Finetuned encoder<br>\n",
        "VectorDB : FAISS<br>\n",
        "LLM : Gemma2-2b-it 기반\n",
        "Finetuned by Q&A and 2019-2023 annaual reports <br>\n",
        "Parkppp/gemma2-BOK -> roomnumber103/LLM-BOK-gemma2\n",
        "<br>\n",
        "\n",
        "for more information : <a href=\"https://docs.google.com/document/d/1zHqC6-eiz2cPY8vopkAEKBnP9I_XkDPi2FSC3UCeqoo/edit?usp=sharing\" >  BOK(Bank of Korea) Reports Analyzer</a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Google Playground/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zjxsend7g1lA",
        "outputId": "669c4270-d379-42b7-f530-d1e70ce1aca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "wxSCNztRnOAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055fb828-2ef9-44e5-b5e8-8c5e02251e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQbz2bLN5KHQ"
      },
      "source": [
        "# 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsDgGxVK5KHS"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas numpy huggingface_hub langchain langchain-community faiss-cpu sentence-transformers torch accelerate datasets accelerate peft trl bitsandbytes pyarrow gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from huggingface_hub import login\n",
        "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Text Splitting\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "from langchain import FAISS\n",
        "import faiss\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from torch.amp import autocast"
      ],
      "metadata": {
        "id": "oSR2-WddnWtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGgiFKn35KHW"
      },
      "source": [
        "# Query-Answer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMnE1PpY5KHW"
      },
      "outputs": [],
      "source": [
        "# 사전정의된 Query-Answer가 담긴 테이블\n",
        "qna_df = pd.read_csv(path + 'qa_data.csv')[['질문', '답변']]\n",
        "\n",
        "qna_df['질문'] = qna_df['질문'].apply(lambda x: x.split('질문\\n')[1]) # \"질문\\n\" 제거\n",
        "qna_df['답변'] = qna_df['답변'].apply(lambda x: x.split('답변\\n')[1]) # \"답변\\n\" 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRIeTlgV5KHX"
      },
      "outputs": [],
      "source": [
        "# SentenceTransformer 모델 로드\n",
        "embedding_model = SentenceTransformer('roomnumber103/embedding-BOK')\n",
        "\n",
        "# 쿼리 문장들에 대한 임베딩 벡터 생성\n",
        "query_texts = qna_df['질문'].to_list()\n",
        "query_embeddings = embedding_model.encode(query_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCImwwwp5KHX"
      },
      "outputs": [],
      "source": [
        "# query-answer 함수 정의\n",
        "def qna_answer_to_query(new_query, embedding_model=embedding_model, query_embeddings=query_embeddings, top_k=1, verbose=True):\n",
        "    # 쿼리 임베딩 계산\n",
        "    new_query_embedding = embedding_model.encode([new_query])\n",
        "\n",
        "\n",
        "    # 코사인 유사도 계산\n",
        "    cos_sim = cosine_similarity(new_query_embedding, query_embeddings)\n",
        "\n",
        "    # 코사인 유사도 값이 가장 큰 질문의 인덱스 찾기\n",
        "    most_similar_idx = np.argmax(cos_sim)\n",
        "    similarity = np.round(cos_sim[0][most_similar_idx], 2)\n",
        "\n",
        "    # 가장 비슷한 질문과 답변 가져오기\n",
        "    similar_query = query_texts[most_similar_idx]\n",
        "    similar_answer = qna_df.iloc[most_similar_idx]['답변']\n",
        "\n",
        "    if verbose == True:\n",
        "        print(\"가장 비슷한 질문 : \", similar_query)\n",
        "        print(\"가장 비슷한 질문의 유사도 : \", similarity)\n",
        "        print(\"가장 비슷한 질문의 답: \", similar_answer)\n",
        "\n",
        "    # 결과 반환\n",
        "    return similar_query, similarity, similar_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Avt0bg5KHY"
      },
      "source": [
        "# RAG Function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loaders\n",
        "text_folder = os.path.join(path, 'text_data')\n",
        "#text_paths = glob.glob(os.path.join(path, '*.txt'))\n",
        "text_paths = [os.path.join(text_folder, file) for file in os.listdir(text_folder)]"
      ],
      "metadata": {
        "id": "Gt7aRH8UUChC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = []\n",
        "\n",
        "for i in range(len(text_paths)):\n",
        "    print(text_paths[i])\n",
        "    loader = TextLoader(text_paths[i])\n",
        "    document_list = loader.load()  # document_list는 리스트 형식으로 반환될 수 있음\n",
        "\n",
        "    # 리스트 안의 텍스트들을 하나의 문자열로 결합\n",
        "    document_text = \" \".join([doc.page_content for doc in document_list])\n",
        "\n",
        "    # Document 객체로 변환하여 documents 리스트에 추가\n",
        "    documents.append(Document(page_content=document_text))\n",
        "\n",
        "# splitter 정의\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \"? \", \"! \"]\n",
        ")\n",
        "\n",
        "# Document 형식의 보고서를 splitter로 분할\n",
        "splitted_documents = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "# 문장이 마침표로 시작하면 마침표 제거, 마침표로 끝나지 않으면 마침표 추가\n",
        "for i in range(len(splitted_documents)):\n",
        "    splitted_documents[i].page_content = splitted_documents[i].page_content.strip('. ').strip()\n",
        "    if not splitted_documents[i].page_content.endswith('.'):\n",
        "        splitted_documents[i].page_content += '.'"
      ],
      "metadata": {
        "id": "Ssx6LgSnUejt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91c1525-2671-45eb-ee70-b5af5e5a2280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2023.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2021.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2020.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2019.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2022.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2023.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2021.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2020.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2019.txt\n",
            "/content/drive/MyDrive/Google Playground/text_data/rectified_text_2022.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yMKIA0xDVJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69030af3-5616-4d07-98a8-4329fa90ad00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "# 처음 FAISS Index를 생성시에는 아래 코드의 주석을 해제하여 사용한다\n",
        "# Document 객체에서 텍스트 추출\n",
        "report_texts = [doc.page_content for doc in splitted_documents]\n",
        "\n",
        "# 문서 임베딩 생성 (embed_documents 대신 encode 사용)\n",
        "#embeddings = embedding_model.encode(report_texts)\n",
        "\n",
        "# FAISS 인덱스 생성\n",
        "dim = 768  # 임베딩 차원 = len(embeddings[0])\n",
        "#report_index = faiss.IndexFlatL2(dim)  # L2 거리 기반의 인덱스 생성\n",
        "#report_index.add(np.array(embeddings))\n",
        "\n",
        "# FAISS 인덱스 저장\n",
        "#faiss.write_index(report_index, os.path.join(path, \"report_index.index\"))\n",
        "\n",
        "# FAISS 인덱스 불러오기\n",
        "loaded_index = faiss.read_index(os.path.join(path, \"report_index.index\"))\n",
        "\n",
        "# 문서 저장소와 매핑 생성\n",
        "docstore = InMemoryDocstore({idx: Document(page_content=text) for idx, text in enumerate(report_texts)})\n",
        "docstore_id_map = {idx: idx for idx in range(len(report_texts))}\n",
        "\n",
        "# embedding_function을 lambda로 수정하여 encode 호출\n",
        "database = FAISS(embedding_function=lambda texts: embedding_model.encode(texts), index=loaded_index, docstore=docstore, index_to_docstore_id=docstore_id_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JWPM-8F5KHa"
      },
      "source": [
        "# Query Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWoY_J-17_4D"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.float32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8RWeB0V7_8R"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.0,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                    \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "# MPS 디바이스 설정\n",
        "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "#모델 설정\n",
        "model_id = \"roomnumber103/LLM-BOK-gemma2\"\n",
        "\n",
        "# 모델 로드(Hub)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# 토크나이저 로드(Hub)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "\n",
        "# 모델 로드(local)\n",
        "#model = AutoModelForCausalLM.from_pretrained(os.path.join(path, \"gemma2_continued_lora/gemma2_continued_lora_model\"))\n",
        "\n",
        "# 토크나이저 로드(Local)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(os.path.join(path,\"gemma2_continued_lora/gemma2_continued_lora_tokenizer\"), use_fast=False)\n",
        "\n",
        "# 모델에 동일한 PEFT (LoRA) 설정 적용\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# 디바이스 설정 및 이동\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a427841443444814bc2eb83ddf51c299",
            "fa7a88cbd755440cbd8864618793860b",
            "4c2d2c889598465bb42734729866b673",
            "c0f7f7b88f1743b088e5a07c466ae718",
            "37a1a33fad4843dd947c7c49e9d4b56c",
            "77bdbf92f290421a85c38330107c948d",
            "07471fd09d934284bed4bcd248a30906",
            "d64e70a2e2c048398ac8c9a9b402f957",
            "a3ce30d2570844d8baf75dbf9eb8c65a",
            "8cce4d7e1425439a9d5274e24d516bb6",
            "441fe03bebb14e7fa3ee027d1118d822",
            "09ed15fa20454f3cac4cbaa2bcbc0761",
            "b1c1b79357fc4920b86dff0eb9bcf196",
            "39e5a45deb1748b8924457a2520dbaef",
            "6281bfb299f34bf386bb5b519fae5a8e",
            "2ff0ce8a03ee45ffa36d67b6fad3fc85",
            "faa8b20cf43740a4ad616381321d5531",
            "db99147cf76b49319ad7b67201f08561",
            "c9c4116a21fe46ab86cd2a639ded5e9e",
            "8cf27654ae9f47afb1e4a0dfd61ff67e",
            "8727d897faaf4dd29a17433ccd2e01e1",
            "53a4c404f7fc4905baa7076a4febaafd"
          ]
        },
        "id": "1-WilrKw2-xu",
        "outputId": "981ec04e-acec-4e40-f107-c9927a182052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a427841443444814bc2eb83ddf51c299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma2ForCausalLM(\n",
              "      (model): Gemma2Model(\n",
              "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-25): 26 x Gemma2DecoderLayer(\n",
              "            (self_attn): Gemma2SdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): Gemma2RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Gemma2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09ed15fa20454f3cac4cbaa2bcbc0761"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma2ForCausalLM(\n",
              "      (model): Gemma2Model(\n",
              "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-25): 26 x Gemma2DecoderLayer(\n",
              "            (self_attn): Gemma2SdpaAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2048, out_features=2304, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): Gemma2RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Gemma2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=2304, out_features=9216, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=9216, out_features=2304, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Identity()\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUHj7E2QBad9"
      },
      "outputs": [],
      "source": [
        "def question_gemma(prompt, model, tokenizer, temperature=0.7, max_new_tokens=512, line_length=80, max_sentences=5):\n",
        "\n",
        "    # 입력 데이터 생성 및 이동\n",
        "    # input_prompt = f\"입력: {prompt}\\n출력: \"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # 답변 생성\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=None,\n",
        "            early_stopping=False\n",
        "        )\n",
        "\n",
        "    # 입력 부분을 제외한 생성된 토큰만 디코딩\n",
        "    generated_tokens = outputs[0][input_ids.shape[-1]:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # 불필요한 '*' 제거\n",
        "    response = re.sub(r\"\\*\\*(.*?)\\*\\*\", r\"\\1\", response)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 프롬프트 템플릿 설정\n",
        "prompt_template = PromptTemplate(\n",
        "    template=\"\"\"경제전문가로서 선행지식과 주어진 정보를 종합하여 답하시오. 1) 영어로 작성하지 말 것. Don't Use English. 2) 144자 정도의 완결된 문장으로 작성하고, 묻는 말에 끝까지 답할 것. 3) 답변에 **을 사용하지 말 것.\n",
        "\n",
        "***Information***\n",
        "{context}\n",
        "\n",
        "***Question***\n",
        "{query}\n",
        "\n",
        "***Answer***\"\"\",\n",
        "    input_variables=['context', 'query']\n",
        ")\n",
        "\n",
        "# 후처리 설정\n",
        "def format_output_text(text, line_length=80):\n",
        "\n",
        "\n",
        "    # 1. 문장 단위로 분리\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "\n",
        "    # 2. 각 문장들을 line_length마다 줄바꿈\n",
        "    formatted_text = \"\"\n",
        "    current_line = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # 현재 줄에 추가하여 줄 길이가 너무 길어지면 줄바꿈 추가\n",
        "        if len(current_line) + len(sentence) < line_length:\n",
        "            current_line += sentence + \" \"\n",
        "        else:\n",
        "            formatted_text += current_line.strip() + \"\\n\"  # 줄바꿈 추가\n",
        "            current_line = sentence + \" \"\n",
        "\n",
        "    formatted_text += current_line.strip()  # 마지막 줄 추가\n",
        "    return formatted_text\n",
        "\n",
        "\n",
        "def trim_response(response, max_sentences=10):\n",
        "    # 문장 단위로 자르기\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', response)\n",
        "    # 최대 문장 수만큼만 유지\n",
        "    return ' '.join(sentences[:max_sentences])\n",
        "\n",
        "# 컨텍스트를 가져와 gemma에서 결과를 받은 뒤 형식을 다듬는 함수\n",
        "def chat_with_retrieval(query, model=model, tokenizer=tokenizer, temperature=0.7, max_new_tokens=512, line_length=80, max_sentences=5):\n",
        "\n",
        "    # 유사도가 높은 문장을 검색\n",
        "    results = database.similarity_search(query, k=5)\n",
        "\n",
        "    #조회를 통해 얻은 정보를 저장\n",
        "    context = \"Relevant documents:\\n\"\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        context += f\"\"\"\n",
        "        -----------------------------------\n",
        "        Doc {i+1}:\n",
        "        {result.page_content}\n",
        "        \"\"\"\n",
        "\n",
        "    # 프롬프트 템플릿 적용\n",
        "    prompt = prompt_template.format(context=context, query=query)\n",
        "\n",
        "    # LLM을 통해 답변 생성\n",
        "    response = question_gemma(prompt=prompt, model=model, tokenizer=tokenizer, temperature=temperature, max_new_tokens=max_new_tokens)  # 여기서는 LLM 호출 결과가 문자열로 반환됩니다.\n",
        "\n",
        "    # 최대 문장 수로 자르기\n",
        "    response = trim_response(response, max_sentences=max_sentences)\n",
        "\n",
        "    # 출력 포맷팅\n",
        "    formatted_response = format_output_text(response, line_length=line_length)\n",
        "\n",
        "    return formatted_response"
      ],
      "metadata": {
        "id": "bKpPjMS1qc7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKRBQOlY5KHb"
      },
      "source": [
        "# Combine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_answer_to_query(new_query, model=model, tokenizer=tokenizer, critical_value=0.5, temperature=0.7, max_new_tokens=512, line_length=80, max_sentences=5):\n",
        "    \"\"\"\n",
        "    주어진 질문에 대해 Q&A 데이터셋에서 유사도를 검사하고, 유사도가 낮을 경우 모델을 통해 답변 생성.\n",
        "    \"\"\"\n",
        "    # Q&A 데이터셋에서 유사 질문 찾기\n",
        "    qna_result = qna_answer_to_query(new_query, verbose=False)\n",
        "    qna_similarity = qna_result[1]\n",
        "    qna_answer = qna_result[2]\n",
        "\n",
        "    if qna_similarity >= critical_value:\n",
        "        # Q&A 데이터셋에 유사 질문이 있는 경우\n",
        "        msg = 'Q&A에 등재된 내용을 기반으로 답변을 구합니다.\\n\\n'\n",
        "        return msg + qna_answer\n",
        "    else:\n",
        "        # 유사 질문이 없는 경우 -> 모델을 통해 답변 생성\n",
        "        msg = '연차보고서를 참고로 Gemma2 모델을 사용하여 답변을 생성합니다.\\n\\n'\n",
        "        generated_answer = chat_with_retrieval(new_query, model=model, tokenizer=tokenizer, temperature=0.7, max_new_tokens=512, line_length=80, max_sentences=5)\n",
        "        return generated_answer"
      ],
      "metadata": {
        "id": "VDw7q0V_gXO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qna_answer_to_query('2023년 통화정책의 운용목표는?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc0apgm0CkXJ",
        "outputId": "f0bea044-2411-4692-e755-a04ecc675979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 비슷한 질문 :  통화정책 최종대부자\n",
            "가장 비슷한 질문의 유사도 :  0.43\n",
            "가장 비슷한 질문의 답:  최종대부자(lender of last resort)란 금융위기로 인해 개별 금융기관 또는 전체 금융시장에 유동성 부족사태가 발생했을 때 위기 극복을 위해 유동성을 공급해 줄 수 있는 마지막 보루를 뜻합니다. 이는 현실적으로 화폐의 독점적 발행권과 무제한 공급능력을 가지고 있는 중앙은행만이 할 수 있는 일입니다.\n",
            "  금융기관, 특히 은행은 일반적으로 단기로 예금을 받아 장기로 대출을 해 주기 때문에 보유하고 있는 자산과 부채의 만기가 일치하지 않습니다. 은행은 고객의 예금인출 요구에는 즉각 부응해야 하지만 대출을 회수하는 데는 시간이 걸립니다. 이에 따라 은행은 예금인출 사태(bank-run)가 발생하는 경우 예금인출 요구에 응하지 못하는 유동성 위기에 처할 가능성이 높습니다. 더욱이 한 은행의 유동성 위기는 금융시스템 전체에 대한 불신으로 이어져 건전한 은행이라도 동반하여 위기에 처해질 수 있습니다. 따라서 중앙은행은 금융위기가 발생하였을 때 지불능력은 있으나 단기적으로 유동성이 부족하여 예금인출 요구에 응할 수 없는 금융기관(solvent but illiquid institutions)에 충분한 자금을 공급해 위기를 넘길 수 있게 도와줌으로써 사람들의 불안 심리를 안정시키고 위기의 확산을 방지하는 역할을 합니다.\n",
            "  한편 글로벌 금융위기 극복과정에서는 중앙은행이 개별 금융기관을 넘어 금융시장 전체에 유동성을 공급하는 정책도 수행하였습니다. 이렇게 전통적인 최종대부자 기능을 보완하는 중앙은행의 정책이 필요하게 된 것은 금융시장이 고도화되면서 회사채나 금융채 발행을 통한 자금조달이 확대되었고 이들 채권이 거래되는 유통시장(secondary market)의 효율적인 작동이 금융시스템 안정에 매우 중요해졌기 때문입니다.\n",
            "  그러나 중앙은행의 최종대부자 기능 수행은 금융기관으로 하여금 고수익-고위험 자산을 더욱 선호하도록 하고 예금자가 금융기관 경영 및 재무상태의 건전성에 대한 판단 없이 더 높은 금리를 제시하는 금융기관에 자금을 맡기려는 성향을 강화시키는 등 도덕적 해이(moral hazard)를 초래할 위험성이 있습니다. 이러한 도덕적 해이를 방지하기 위해 중앙은행은 필요한 경우 경영실태 파악과 지도·감독 등을 통하여 금융기관이 건전하게 경영될 수 있도록 유도하는 역할을 수행하기도 합니다.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('통화정책 최종대부자',\n",
              " 0.43,\n",
              " '최종대부자(lender of last resort)란 금융위기로 인해 개별 금융기관 또는 전체 금융시장에 유동성 부족사태가 발생했을 때 위기 극복을 위해 유동성을 공급해 줄 수 있는 마지막 보루를 뜻합니다. 이는 현실적으로 화폐의 독점적 발행권과 무제한 공급능력을 가지고 있는 중앙은행만이 할 수 있는 일입니다.\\n  금융기관, 특히 은행은 일반적으로 단기로 예금을 받아 장기로 대출을 해 주기 때문에 보유하고 있는 자산과 부채의 만기가 일치하지 않습니다. 은행은 고객의 예금인출 요구에는 즉각 부응해야 하지만 대출을 회수하는 데는 시간이 걸립니다. 이에 따라 은행은 예금인출 사태(bank-run)가 발생하는 경우 예금인출 요구에 응하지 못하는 유동성 위기에 처할 가능성이 높습니다. 더욱이 한 은행의 유동성 위기는 금융시스템 전체에 대한 불신으로 이어져 건전한 은행이라도 동반하여 위기에 처해질 수 있습니다. 따라서 중앙은행은 금융위기가 발생하였을 때 지불능력은 있으나 단기적으로 유동성이 부족하여 예금인출 요구에 응할 수 없는 금융기관(solvent but illiquid institutions)에 충분한 자금을 공급해 위기를 넘길 수 있게 도와줌으로써 사람들의 불안 심리를 안정시키고 위기의 확산을 방지하는 역할을 합니다.\\n  한편 글로벌 금융위기 극복과정에서는 중앙은행이 개별 금융기관을 넘어 금융시장 전체에 유동성을 공급하는 정책도 수행하였습니다. 이렇게 전통적인 최종대부자 기능을 보완하는 중앙은행의 정책이 필요하게 된 것은 금융시장이 고도화되면서 회사채나 금융채 발행을 통한 자금조달이 확대되었고 이들 채권이 거래되는 유통시장(secondary market)의 효율적인 작동이 금융시스템 안정에 매우 중요해졌기 때문입니다.\\n  그러나 중앙은행의 최종대부자 기능 수행은 금융기관으로 하여금 고수익-고위험 자산을 더욱 선호하도록 하고 예금자가 금융기관 경영 및 재무상태의 건전성에 대한 판단 없이 더 높은 금리를 제시하는 금융기관에 자금을 맡기려는 성향을 강화시키는 등 도덕적 해이(moral hazard)를 초래할 위험성이 있습니다. 이러한 도덕적 해이를 방지하기 위해 중앙은행은 필요한 경우 경영실태 파악과 지도·감독 등을 통하여 금융기관이 건전하게 경영될 수 있도록 유도하는 역할을 수행하기도 합니다.')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가장 비슷한 질문 :  통화정책 최종대부자\n",
            "가장 비슷한 질문의 유사도 :  0.43\n",
            "가장 비슷한 질문의 답:  최종대부자(lender of last resort)란 금융위기로 인해 개별 금융기관 또는 전체 금융시장에 유동성 부족사태가 발생했을 때 위기 극복을 위해 유동성을 공급해 줄 수 있는 마지막 보루를 뜻합니다. 이는 현실적으로 화폐의 독점적 발행권과 무제한 공급능력을 가지고 있는 중앙은행만이 할 수 있는 일입니다.\n",
            "  금융기관, 특히 은행은 일반적으로 단기로 예금을 받아 장기로 대출을 해 주기 때문에 보유하고 있는 자산과 부채의 만기가 일치하지 않습니다. 은행은 고객의 예금인출 요구에는 즉각 부응해야 하지만 대출을 회수하는 데는 시간이 걸립니다. 이에 따라 은행은 예금인출 사태(bank-run)가 발생하는 경우 예금인출 요구에 응하지 못하는 유동성 위기에 처할 가능성이 높습니다. 더욱이 한 은행의 유동성 위기는 금융시스템 전체에 대한 불신으로 이어져 건전한 은행이라도 동반하여 위기에 처해질 수 있습니다. 따라서 중앙은행은 금융위기가 발생하였을 때 지불능력은 있으나 단기적으로 유동성이 부족하여 예금인출 요구에 응할 수 없는 금융기관(solvent but illiquid institutions)에 충분한 자금을 공급해 위기를 넘길 수 있게 도와줌으로써 사람들의 불안 심리를 안정시키고 위기의 확산을 방지하는 역할을 합니다.\n",
            "  한편 글로벌 금융위기 극복과정에서는 중앙은행이 개별 금융기관을 넘어 금융시장 전체에 유동성을 공급하는 정책도 수행하였습니다. 이렇게 전통적인 최종대부자 기능을 보완하는 중앙은행의 정책이 필요하게 된 것은 금융시장이 고도화되면서 회사채나 금융채 발행을 통한 자금조달이 확대되었고 이들 채권이 거래되는 유통시장(secondary market)의 효율적인 작동이 금융시스템 안정에 매우 중요해졌기 때문입니다.\n",
            "  그러나 중앙은행의 최종대부자 기능 수행은 금융기관으로 하여금 고수익-고위험 자산을 더욱 선호하도록 하고 예금자가 금융기관 경영 및 재무상태의 건전성에 대한 판단 없이 더 높은 금리를 제시하는 금융기관에 자금을 맡기려는 성향을 강화시키는 등 도덕적 해이(moral hazard)를 초래할 위험성이 있습니다. 이러한 도덕적 해이를 방지하기 위해 중앙은행은 필요한 경우 경영실태 파악과 지도·감독 등을 통하여 금융기관이 건전하게 경영될 수 있도록 유도하는 역할을 수행하기도 합니다.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('통화정책 최종대부자',\n",
              " 0.43,\n",
              " '최종대부자(lender of last resort)란 금융위기로 인해 개별 금융기관 또는 전체 금융시장에 유동성 부족사태가 발생했을 때 위기 극복을 위해 유동성을 공급해 줄 수 있는 마지막 보루를 뜻합니다. 이는 현실적으로 화폐의 독점적 발행권과 무제한 공급능력을 가지고 있는 중앙은행만이 할 수 있는 일입니다.\\n  금융기관, 특히 은행은 일반적으로 단기로 예금을 받아 장기로 대출을 해 주기 때문에 보유하고 있는 자산과 부채의 만기가 일치하지 않습니다. 은행은 고객의 예금인출 요구에는 즉각 부응해야 하지만 대출을 회수하는 데는 시간이 걸립니다. 이에 따라 은행은 예금인출 사태(bank-run)가 발생하는 경우 예금인출 요구에 응하지 못하는 유동성 위기에 처할 가능성이 높습니다. 더욱이 한 은행의 유동성 위기는 금융시스템 전체에 대한 불신으로 이어져 건전한 은행이라도 동반하여 위기에 처해질 수 있습니다. 따라서 중앙은행은 금융위기가 발생하였을 때 지불능력은 있으나 단기적으로 유동성이 부족하여 예금인출 요구에 응할 수 없는 금융기관(solvent but illiquid institutions)에 충분한 자금을 공급해 위기를 넘길 수 있게 도와줌으로써 사람들의 불안 심리를 안정시키고 위기의 확산을 방지하는 역할을 합니다.\\n  한편 글로벌 금융위기 극복과정에서는 중앙은행이 개별 금융기관을 넘어 금융시장 전체에 유동성을 공급하는 정책도 수행하였습니다. 이렇게 전통적인 최종대부자 기능을 보완하는 중앙은행의 정책이 필요하게 된 것은 금융시장이 고도화되면서 회사채나 금융채 발행을 통한 자금조달이 확대되었고 이들 채권이 거래되는 유통시장(secondary market)의 효율적인 작동이 금융시스템 안정에 매우 중요해졌기 때문입니다.\\n  그러나 중앙은행의 최종대부자 기능 수행은 금융기관으로 하여금 고수익-고위험 자산을 더욱 선호하도록 하고 예금자가 금융기관 경영 및 재무상태의 건전성에 대한 판단 없이 더 높은 금리를 제시하는 금융기관에 자금을 맡기려는 성향을 강화시키는 등 도덕적 해이(moral hazard)를 초래할 위험성이 있습니다. 이러한 도덕적 해이를 방지하기 위해 중앙은행은 필요한 경우 경영실태 파악과 지도·감독 등을 통하여 금융기관이 건전하게 경영될 수 있도록 유도하는 역할을 수행하기도 합니다.')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def context_documents(query, k=5):\n",
        "    contexts = [doc.page_content for doc in database.similarity_search('2023년 통화정책의 운용목표는?', k=5)]\n",
        "    for i, context in enumerate(contexts):\n",
        "        print(f'Document {i+1}:\\n{context}\\n')\n",
        "\n",
        "context_documents('2023년 통화정책의 운용목표는?', k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VPLyMlHGZVz",
        "outputId": "ea0b9656-a345-4420-840a-1af6f3b29836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1:\n",
            "2023년 통화신용정책 운영방향.\n",
            "\n",
            "Document 2:\n",
            "한국은행은 중기적 시계에서 소비자 물가 상승률이 물가 안정 목표에 근접하도록 통화신용정책을 운영하고 있다. 2023년 중 소비자 물가 상승률은 국제 원자재 가격 하락, 통화 긴축에 따른 수요 측 물가 압력 완화 등의 영향으로 3.6%로 낮아졌으나 2022년에 이어 물가 안정 목표를 상회하였다.\n",
            "\n",
            "Document 3:\n",
            "나. 기준금리 한국은행은 2023년 중 성장세를 점검하면서 중기적 시계에서 물가 상승률이 목표 수준(2%)에서 안정될 수 있도록 기준금리를 긴축적인 수준에서 운용하였다.\n",
            "\n",
            "Document 4:\n",
            "이 과정에서 코로나19의 전개 및 주요국의 경기 상황 등을 점검하는 한편, 자산시장으로의 자금 쏠림, 가계 부채 증가 등 금융 불균형 누적에 보다 유의할 것이다.\n",
            "\n",
            "통화정책 방향에서는 금융통화위원회가 다음 통화정책방향 결정 시까지 한국은행 기준금리를 현 수준(0.50%)에서 유지하여 통화정책을 운용하기로 하였다.\n",
            "\n",
            "Document 5:\n",
            "아울러 2023년 2월부터 2025년까지 단계적으로 100%로 인상하는 계획을 발표하였다.\n",
            "\n",
            "감시 및 리스크 관리 금융시장 인프라에 대한 평가 한국은행은 지급결제 분야의 국제 기준에 의거하여 중요 지급결제시스템에 대한 평가를 실시하고 미비점에 대해서는 개선을 권고하고 있다.\n",
            "\n",
            "Document 1:\n",
            "2023년 통화신용정책 운영방향.\n",
            "\n",
            "Document 2:\n",
            "한국은행은 중기적 시계에서 소비자 물가 상승률이 물가 안정 목표에 근접하도록 통화신용정책을 운영하고 있다. 2023년 중 소비자 물가 상승률은 국제 원자재 가격 하락, 통화 긴축에 따른 수요 측 물가 압력 완화 등의 영향으로 3.6%로 낮아졌으나 2022년에 이어 물가 안정 목표를 상회하였다.\n",
            "\n",
            "Document 3:\n",
            "나. 기준금리 한국은행은 2023년 중 성장세를 점검하면서 중기적 시계에서 물가 상승률이 목표 수준(2%)에서 안정될 수 있도록 기준금리를 긴축적인 수준에서 운용하였다.\n",
            "\n",
            "Document 4:\n",
            "이 과정에서 코로나19의 전개 및 주요국의 경기 상황 등을 점검하는 한편, 자산시장으로의 자금 쏠림, 가계 부채 증가 등 금융 불균형 누적에 보다 유의할 것이다.\n",
            "\n",
            "통화정책 방향에서는 금융통화위원회가 다음 통화정책방향 결정 시까지 한국은행 기준금리를 현 수준(0.50%)에서 유지하여 통화정책을 운용하기로 하였다.\n",
            "\n",
            "Document 5:\n",
            "아울러 2023년 2월부터 2025년까지 단계적으로 100%로 인상하는 계획을 발표하였다.\n",
            "\n",
            "감시 및 리스크 관리 금융시장 인프라에 대한 평가 한국은행은 지급결제 분야의 국제 기준에 의거하여 중요 지급결제시스템에 대한 평가를 실시하고 미비점에 대해서는 개선을 권고하고 있다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(question_gemma('2023년 통화정책의 운용목표는?', model=model, tokenizer=tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gdd3AixGsR8",
        "outputId": "303b8833-30e4-4f7a-a1ac-80e45be07088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "답변: \n",
            "\n",
            "2023년 통화정책의 운용목표는 국가 경제 성장 및 안정적인 금리 상승을 이루어내는 것입니다.  \n",
            "\n",
            "\n",
            "좀 더 자세한 설명:\n",
            "\n",
            "* 국가 경제 성장: 2023년에는 국내 경제 활성화를 위한 다양한 정책이 시행될 예정이며, 이는 경제 성장에 기여하는 중요한 요소일 것입니다.\n",
            "* 안정적인 금리 상승: 높은 금리가 국민의 소비를 저해하고 투자를 방해할 수 있으므로, 통화 정책은 금리를 안정적으로 높여야 합니다.\n",
            "\n",
            "\n",
            "추가적인 고려 사항:\n",
            "\n",
            "* 전체적인 경제 상황: 국내외 경제 상황과 관련하여 통화 정책의 운용 목표와 전략을 조정해야 할 수도 있습니다.\n",
            "* 금리 수준: 현재의 금리 수준과 미래의 금리 수준을 고려하여 통화 정책을 결정해야 합니다. \n",
            "\n",
            "\n",
            "결론적으로, 2023년 통화정책의 운용목표는 국가 경제 성장과 안정적인 금리 상승을 동시에 달성하는 것이며, 이를 위해서는 경제 상황 변화에 대한 적절한 대응과 지속적인 모니터링이 필요합니다.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "답: \n",
            "- 국가 경제 성장 및 안전한 금융 시스템 유지: \n",
            "  * 국내 기업 활성화, 부채 관리, 불안정한 환경 완화 등을 목표로 하는 통화 정책은 국가 경제 성장과 안전한 금융 시스템 유지를 위한 필수적인 요소이다.\n",
            "\n",
            "\n",
            "추가 설명:\n",
            "\n",
            "* 국가 경제 성장: 통화 정책은 국가 경제 성장에 큰 영향을 미치며, 이를 통해 높은 수입, 고용 증가, 그리고 소비 활발화를 이끌어낼 수 있다.\n",
            "* 안정한 금융 시스템 유지: 통화 정책은 금리 조절, 자본 이동, 투자 유동 등으로 금융 시스템의 안정성을 유지하는 데 중요한 역할을 한다.\n",
            "\n",
            "\n",
            "결론적으로 2023년 통화정책의 운용목표는 국가 경제 성장 및 안전한 금융 시스템 유지라는 두 가지 주요 목표를 달성하기 위해 적극적으로 실행되고 있는 것이다. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_answer_to_query('2023년 통화정책의 운용목표는?', model=model, tokenizer=tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqgWGee7H9X1",
        "outputId": "5c99829f-8a50-459c-c744-f315d82c1951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            ".\n",
            ". .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_with_retrieval('2023년 통화정책의 운용목표는?', model=model, tokenizer=tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qjn401hxKpV",
        "outputId": "053f372b-7ba0-4aec-ee3e-e7a1fa5fc1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "한국은행은 2023년 통화정책을 운용하면서 중기적 시계에서 소비자 물가 상승률이 물가 안정 목표에 근접하도록 하는 것이었다.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X7p4TFJ5KHc"
      },
      "source": [
        "# Web Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tV-l8tFR5KHc"
      },
      "outputs": [],
      "source": [
        "def chat_with_full_answer_to_query(message, history):\n",
        "    # 사용자의 질문에 대해 full_answer_to_query를 사용하여 답변 생성\n",
        "    response = full_answer_to_query(message, model, tokenizer)\n",
        "\n",
        "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
        "    history.append((message, response))\n",
        "\n",
        "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
        "    return history, history\n",
        "\n",
        "def chat_with_full_qna(message, history):\n",
        "    # 사용자의 질문에 대해 qna_answer_to_query를 사용하여 답변 생성\n",
        "    response = qna_answer_to_query(message)\n",
        "\n",
        "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
        "    history.append((message, response))\n",
        "\n",
        "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
        "    return history, history\n",
        "\n",
        "def get_context_documents(message, history):\n",
        "    # 사용자의 질문에 대해 context_documents를 사용하여 관련 문서를 반환\n",
        "    response = context_documents(message, k=5)\n",
        "\n",
        "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
        "    history.append((message, response))\n",
        "\n",
        "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
        "    return history, history\n",
        "\n",
        "def chat_with_gemma(message, history):\n",
        "        # 사용자의 질문에 대해 사전학습된 모델만을 사용하여 답변 생성\n",
        "    response = question_gemma(message, model=model, tokenizer=tokenizer)\n",
        "\n",
        "    # 질문과 답변을 히스토리에 저장 (history는 대화 히스토리)\n",
        "    history.append((message, response))\n",
        "\n",
        "    # Gradio가 (응답, history)를 반환해야 하므로, 대화 기록과 함께 반환\n",
        "    return history, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAxsQfeU5KHc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "0bec67cc-3a6e-4457-ced3-c8244ae91f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://c1ebc28ec2ab753274.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c1ebc28ec2ab753274.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Gradio Chatbot 인터페이스 생성\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot()  # 대화 기록을 표시하는 컴포넌트\n",
        "    msg = gr.Textbox(label=\"질문 입력\")  # 질문 입력을 위한 텍스트 박스\n",
        "    clear = gr.Button(\"대화 기록 초기화\")  # 대화 기록 초기화 버튼\n",
        "\n",
        "    # 대화가 시작될 때 실행할 동작 정의\n",
        "    def clear_history():\n",
        "        return []\n",
        "\n",
        "    #msg.submit(chat_with_full_answer_to_query, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
        "    msg.submit(chat_with_gemma, inputs=[msg, chatbot], outputs=[chatbot, chatbot])\n",
        "\n",
        "    # 기록 초기화 버튼 동작 정의\n",
        "    clear.click(clear_history, None, chatbot, queue=False)\n",
        "\n",
        "# 앱 실행\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a427841443444814bc2eb83ddf51c299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa7a88cbd755440cbd8864618793860b",
              "IPY_MODEL_4c2d2c889598465bb42734729866b673",
              "IPY_MODEL_c0f7f7b88f1743b088e5a07c466ae718"
            ],
            "layout": "IPY_MODEL_37a1a33fad4843dd947c7c49e9d4b56c"
          }
        },
        "fa7a88cbd755440cbd8864618793860b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77bdbf92f290421a85c38330107c948d",
            "placeholder": "​",
            "style": "IPY_MODEL_07471fd09d934284bed4bcd248a30906",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4c2d2c889598465bb42734729866b673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d64e70a2e2c048398ac8c9a9b402f957",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3ce30d2570844d8baf75dbf9eb8c65a",
            "value": 2
          }
        },
        "c0f7f7b88f1743b088e5a07c466ae718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cce4d7e1425439a9d5274e24d516bb6",
            "placeholder": "​",
            "style": "IPY_MODEL_441fe03bebb14e7fa3ee027d1118d822",
            "value": " 2/2 [00:02&lt;00:00,  1.20s/it]"
          }
        },
        "37a1a33fad4843dd947c7c49e9d4b56c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77bdbf92f290421a85c38330107c948d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07471fd09d934284bed4bcd248a30906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d64e70a2e2c048398ac8c9a9b402f957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ce30d2570844d8baf75dbf9eb8c65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cce4d7e1425439a9d5274e24d516bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "441fe03bebb14e7fa3ee027d1118d822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09ed15fa20454f3cac4cbaa2bcbc0761": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1c1b79357fc4920b86dff0eb9bcf196",
              "IPY_MODEL_39e5a45deb1748b8924457a2520dbaef",
              "IPY_MODEL_6281bfb299f34bf386bb5b519fae5a8e"
            ],
            "layout": "IPY_MODEL_2ff0ce8a03ee45ffa36d67b6fad3fc85"
          }
        },
        "b1c1b79357fc4920b86dff0eb9bcf196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faa8b20cf43740a4ad616381321d5531",
            "placeholder": "​",
            "style": "IPY_MODEL_db99147cf76b49319ad7b67201f08561",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "39e5a45deb1748b8924457a2520dbaef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9c4116a21fe46ab86cd2a639ded5e9e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8cf27654ae9f47afb1e4a0dfd61ff67e",
            "value": 2
          }
        },
        "6281bfb299f34bf386bb5b519fae5a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8727d897faaf4dd29a17433ccd2e01e1",
            "placeholder": "​",
            "style": "IPY_MODEL_53a4c404f7fc4905baa7076a4febaafd",
            "value": " 2/2 [00:02&lt;00:00,  1.10s/it]"
          }
        },
        "2ff0ce8a03ee45ffa36d67b6fad3fc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faa8b20cf43740a4ad616381321d5531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db99147cf76b49319ad7b67201f08561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9c4116a21fe46ab86cd2a639ded5e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cf27654ae9f47afb1e4a0dfd61ff67e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8727d897faaf4dd29a17433ccd2e01e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a4c404f7fc4905baa7076a4febaafd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}